{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ur34g7Fbi5lr"
   },
   "source": [
    "# Notebook- Data Augmentation ( Model K3_2 ) \n",
    "# Author : V.Albors   Date : 05.02.2020\n",
    "# Purpose : Preprocess and Train the CNN (Data Augmentation )\n",
    "# without regularization\n",
    "\n",
    "\n",
    "**Input** :  \n",
    "  * CSV files that identify the images to use as train and validation. CSV files are in directory csv_dir   \n",
    "  * Images from train and validation. Images are in directory : imag_dir  \n",
    "  \n",
    "  \n",
    "**Output**:  \n",
    "  * Download of the model trained with train dataset - with Data Augmentation\n",
    "  * Download the history of the model in order to be evaluated \n",
    "\n",
    "**Process**:  \n",
    " * Read Train and Validation images ( identified in the .csv files ) from the imag_dir directory   \n",
    " * Define Network \n",
    " * Print Network + Save Network Definition\n",
    " * Compile Network \n",
    " * Create a train and validation generator   \n",
    " * Train the model with the train dataset with 100 epochs ( with data Augmentation. Callbacks :ModelCheckpoint)  \n",
    " * Save the trained model and history of the model in directory model_bin_dir \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.keras.backend.clear_session()  # Reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.config.experimental.list_physical_devices('GPU') \n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "10.0\n",
      "7.6\n"
     ]
    }
   ],
   "source": [
    "#Tensorflow version \n",
    "print(tf.__version__)\n",
    "from tensorflow.python.platform import build_info as tf_build_info\n",
    "print(tf_build_info.cuda_version_number)\n",
    "# Cuda Version 9.0 in v1.10.0\n",
    "print(tf_build_info.cudnn_version_number)\n",
    "# CudNN 7 in v1.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the model, directories & if to train the model \n",
    "Model_directory = \"MODELK3\"\n",
    "Model_name = \"ModelK3_2\"\n",
    "TRAIN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['confusion_ROC_AUC', 'create_column_tensor', 'create_label_tensor', 'create_val_test', 'define_dirs', 'extract_images_bm', 'extract_images_train', 'load_hist_model', 'load_images', 'load_images_tf', 'model_load', 'plot_save_acc_loss', 'print_network', 'process_clinical_info', 'read_dataframes', 'read_dataframes_tables', 'reproducible_results', 'save_model', 'save_network_json', 'start', 'stop', 'to_one_hot', 'to_one_hot_words', 'xi_squared']\n"
     ]
    }
   ],
   "source": [
    "# Import routines\n",
    "import sys  \n",
    "subrc_dir = \"/home/valborsf/Documents/UOC/PFMProject/\"\n",
    "sys.path.append(subrc_dir)  \n",
    "from  Models_routines import *\n",
    "import inspect\n",
    "\n",
    "# List functions inside the module\n",
    "import Models_routines as module\n",
    "functions = inspect.getmembers(module, inspect.isfunction)\n",
    "lsfunctions = [item[0] for item in functions]\n",
    "print ( lsfunctions )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducible results \n",
    "reproducible_results ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories\n",
    "(root_dir,json_dir,imag_dir,csv_dir,model_json_dir,model_bin_dir,results_dir,Tensor_dir) = define_dirs(Model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New dataset without SONIC disturbing images\n",
    "json_dir =  root_dir +\"/DataNew/ALL_JSON/\"                # .json dir images\n",
    "imag_dir =  root_dir +\"/DataNew/ALL_IMAGES/\"              # .png dir - images\n",
    "\n",
    "# directories for  CSV's\n",
    "csv_dir =  root_dir +\"/DataNew4/CSV/\"                      # .csv dir - dftrain, dfval, dftest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Add in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarIsInitializedOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Assert in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "#from keras import layers\n",
    "#from keras import models\n",
    "model = models.Sequential ()\n",
    "model.add(layers.Conv2D(64, (3,3), activation='relu', input_shape=(150,150,3)))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(32, (3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(128, (3,3),activation='relu'))\n",
    "model.add(layers.MaxPooling2D((4,4)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dropout(0.23))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 148, 148, 64)      1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 74, 74, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 72, 72, 32)        18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 34, 34, 128)       36992     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 4,317,857\n",
      "Trainable params: 4,317,857\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print Network \n",
    "print_network (results_dir, model, Model_name)\n",
    "#Save Network \n",
    "save_network_json (model_json_dir, model, Model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Compile Network \n",
    "#from keras import optimizers \n",
    "from tensorflow.keras import optimizers\n",
    "model.compile ( loss='binary_crossentropy',\n",
    "#               optimizer = optimizers.RMSprop(lr=1e-4),\n",
    "#                optimizer = optimizers.RMSprop(lr=1e-5),\n",
    "                optimizer = optimizers.Adam(lr=1e-4),\n",
    "               metrics= ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/valborsf/Documents/UOC/PFMProject/DataNew4/CSV/\n"
     ]
    }
   ],
   "source": [
    "# Load train,validation & Test \n",
    "(dftrain, dfval, dftest) = read_dataframes(csv_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbackt to be used \n",
    "#import keras\n",
    "#from keras.callbacks import ModelCheckpoint\n",
    "#from keras.callbacks import EarlyStopping\n",
    "\n",
    "callbacks_list = [\n",
    "         tf.keras.callbacks.EarlyStopping (\n",
    "             monitor = 'val_loss',             # Monitors the accuracy\n",
    "             verbose=1,                        # log when finishes\n",
    "             patience = 4,),              # Interrupt if acc no improve in 3 epochs\n",
    "\n",
    "#  ModelCheckpoint to store the weights of the best performing epoch. \n",
    "    \n",
    "         tf.keras.callbacks.ModelCheckpoint(filepath=model_bin_dir+\"Best_weights\"+Model_name+\".hdf5\", \n",
    "             monitor = 'val_loss', # Won't overwritte the model file unless val_loss has\n",
    "             verbose=1,            # improve \n",
    "             save_best_only=True),\n",
    "         \n",
    "#         keras.callbacks.TensorBoard(\n",
    "#             log_dir =  Tensor_dir, \n",
    "#            histogram_freq = 0,  ) # No histograms - validation data must be provided as a tensor\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3126,
     "status": "ok",
     "timestamp": 1572217564583,
     "user": {
      "displayName": "Victoria Albors",
      "photoUrl": "",
      "userId": "00745240505511363037"
     },
     "user_tz": -60
    },
    "id": "8UzGZWV_i5mX",
    "outputId": "7543fba5-0b25-4af2-eb57-716cb6ad1bac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2100 validated image filenames belonging to 2 classes.\n",
      "Found 700 validated image filenames belonging to 2 classes.\n",
      "Found 700 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "#Rescale images  1/255\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range =40,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True, ) \n",
    "                                   \n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "#batch size = 32 . In this way \"steps per epoch\" ( how many batches have to been treated before going to \n",
    "# the next epoch  is exact  2000 samples =  20 samples x batch  and steps per epoch = 100 - 1 epoch)\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "        dataframe=dftrain,         # Data frame with info of the files and targets\n",
    "        directory=imag_dir,        # path to the directory to read the images from\n",
    "        x_col='file_name_ext',     # column in the data frame that contains the file names \n",
    "        y_col='bm',                # column in the data frame that has the target data\n",
    "        target_size=(150, 150),    # dimensions that the images will be resized\n",
    "        batch_size=128,             # size of the batches of data (default: 32).\n",
    "        class_mode='binary')       # Mode for yielding the targets:1D numpy array of binary labels\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_dataframe(\n",
    "        dataframe=dfval,           # Data frame with info of the files and targets\n",
    "        directory=imag_dir,        # path to the directory to read the images from\n",
    "        x_col='file_name_ext',     # column in the data frame that contains the file names \n",
    "        y_col='bm',                # column in the data frame that has the target data\n",
    "        target_size=(150, 150),    # dimensions that the images will be resized\n",
    "        batch_size=128,             # size of the batches of data (default: 32).\n",
    "        class_mode='binary')       # Mode for yielding the targets:1D numpy array of binary labels\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "        dataframe=dftest,         # Data frame with info of the files and targets\n",
    "        directory=imag_dir,        # path to the directory to read the images from\n",
    "        x_col='file_name_ext',     # column in the data frame that contains the file names \n",
    "        y_col='bm',                # column in the data frame that has the target data\n",
    "        target_size=(150, 150),    # dimensions that the images will be resized\n",
    "        batch_size=128,             # size of the batches of data (default: 32).\n",
    "        shuffle=False,             # IMPORTANT !!! Do not shuffle test data !!!!!!!!!!!!!\n",
    "        class_mode='binary')       # Mode for yielding the targets:1D numpy array of binary labels\n",
    "#        class_mode=None)       # Mode for yielding the targets:1D numpy array of binary labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 20824275,
     "status": "ok",
     "timestamp": 1572238403431,
     "user": {
      "displayName": "Victoria Albors",
      "photoUrl": "",
      "userId": "00745240505511363037"
     },
     "user_tz": -60
    },
    "id": "uCoVQU7Xi5mj",
    "outputId": "4f4f09d8-0b79-430f-cdc2-e5e6389a0eac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    }
   ],
   "source": [
    "if TRAIN :\n",
    "    epochs = 200\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    history = model.fit_generator ( \n",
    "      train_generator,\n",
    "      steps_per_epoch =17,          # 2100 / 128 = 16,6    \n",
    "#      steps_per_epoch =41,                      # nº samples training/ Batch size  = 2614 / 64 \n",
    "      epochs = epochs,\n",
    "      callbacks=callbacks_list,                 # callbacks\n",
    "      validation_data= validation_generator,\n",
    "      validation_steps =6 )        # 700 / 128 = 44 =5,46\n",
    "#      validation_steps =14 )                     # nº samples validation / Batch size = 871 /64\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    print( time.strftime('Time spent in training :'\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "\n",
    "    save_model(model, history, model_bin_dir, Model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Model Test if not need to Train \n",
    "if not TRAIN :\n",
    "    model = model_load ( model_bin_dir, Model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display curves of loss and accuracy during training and save results \n",
    "plot_save_acc_loss(results_dir, history.history, Model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model \n",
    "# Load weights \n",
    "\n",
    "#model = build_model()\n",
    "#model.load_weights('my_weights.model')\n",
    "TRAIN = False\n",
    "if not TRAIN :\n",
    "    Model_name = \"ModelK3_2\"\n",
    "    model = model_load ( model_bin_dir, Model_name)\n",
    "    model.load_weights(model_bin_dir+\"Best_weights\"+Model_name+\".hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4 - ROC /AUC\n",
    "#Evaluate your model  -> Evaluation number -> nº samples/ size batch times\n",
    "#scores = model.evaluate(test_generator, verbose=1, steps = 31)\n",
    "scores = model.evaluate(test_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss = 70 % \n",
    "# Accuracy of 75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predictions\n",
    "# This takes time !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "y_pred_keras = model.predict(test_generator).ravel()       # y_pred_probabilities\n",
    "y_pred = y_pred_keras > 0.5                                # y_pred_class : if >0.5  = True => Malignant\n",
    "y_test = test_generator.classes                            # Ground truth\n",
    "class_labels = list(test_generator.class_indices.keys())   # Class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print Metrics + Confusion ROC AUC\n",
    "confusion_ROC_AUC ( y_test, y_pred, y_pred_keras, class_labels, results_dir, Model_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()  # Reset"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Preprocess_Fit2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "“gpu2”",
   "language": "python",
   "name": "gpu2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
