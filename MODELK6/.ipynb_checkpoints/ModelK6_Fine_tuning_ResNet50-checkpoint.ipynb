{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ur34g7Fbi5lr"
   },
   "source": [
    "# Notebook- Data Augmentation ( Model K6_1 ) Finuning ResNet 50\n",
    "# Author : V.Albors   Date : 08.02.2020\n",
    "# Purpose : Fine Tuning\n",
    "\n",
    "\n",
    "**Input** :  \n",
    "  * CSV files that identify the images to use as train and validation. CSV files are in directory csv_dir   \n",
    "  * Images from train and validation. Images are in directory : imag_dir  \n",
    "  * Saved model. Model is in directory : model_bin_dir  \n",
    "  \n",
    "**Output**:  \n",
    "  * Download of the model trained with train dataset - with Data Augmentation\n",
    "  * Download the history of the model in order to be evaluated \n",
    "\n",
    "**Process**:  \n",
    " * Read Train and Validation images ( identified in the .csv files ) from the imag_dir directory   \n",
    " * Import Pre-Trained Network ( VGG16 or ResNett50 ) \n",
    " \n",
    " \n",
    " * Add custom network on top already trained network\n",
    " * Freeze the base Network\n",
    " * Train the part Added  ( Train with Augmentation + No callback ) \n",
    " \n",
    " * Fine Tune : \n",
    " * Unfreeze some layers in the base network\n",
    " * Jointly train both these layers and the part added ( Train with Augmentation + No callback ) \n",
    " \n",
    " \n",
    " * Save the trained model and history of the model in directory model_bin_dir \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.keras.backend.clear_session()  # Reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.config.experimental.list_physical_devices('GPU') \n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorflow version \n",
    "print(tf.__version__)\n",
    "from tensorflow.python.platform import build_info as tf_build_info\n",
    "print(tf_build_info.cuda_version_number)\n",
    "# Cuda Version 9.0 in v1.10.0\n",
    "print(tf_build_info.cudnn_version_number)\n",
    "# CudNN 7 in v1.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the model, directories & if to train the model \n",
    "#Pre_model = \"VGG16\"\n",
    "Pre_model = \"ResNet50\"\n",
    "if Pre_model == \"ResNet50\":\n",
    "    Model_name = \"ModelK6_1_ResNet50\"\n",
    "else:\n",
    "    Model_name = \"ModelK6_1_VGG16\"\n",
    "TRAIN = True\n",
    "Model_directory = \"MODELK6\"\n",
    "\n",
    "TRAIN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['confusion_ROC_AUC', 'create_column_tensor', 'create_label_tensor', 'define_dirs', 'load_hist_model', 'load_images', 'model_load', 'plot_save_acc_loss', 'print_network', 'process_clinical_info', 'read_dataframes', 'save_model', 'save_network_json', 'to_one_hot', 'to_one_hot_words']\n"
     ]
    }
   ],
   "source": [
    "# Import routines\n",
    "import sys  \n",
    "subrc_dir = \"/home/valborsf/Documents/UOC/PFMProject/\"\n",
    "\n",
    "sys.path.append(subrc_dir)  \n",
    "from  Models_routines import *\n",
    "import inspect\n",
    "\n",
    "# List functions inside the module\n",
    "import Models_routines as module\n",
    "functions = inspect.getmembers(module, inspect.isfunction)\n",
    "lsfunctions = [item[0] for item in functions]\n",
    "print ( lsfunctions )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories\n",
    "(root_dir,json_dir,imag_dir,csv_dir,model_json_dir,model_bin_dir,results_dir,Tensor_dir) = define_dirs(Model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New dataset without SONIC disturbing images\n",
    "json_dir =  root_dir +\"/DataNew/ALL_JSON/\"                # .json dir images\n",
    "imag_dir =  root_dir +\"/DataNew/ALL_IMAGES/\"              # .png dir - images\n",
    "\n",
    "# directories for  CSV's\n",
    "csv_dir =  root_dir +\"/DataNew4/CSV/\"                      # .csv dir - dftrain, dfval, dftest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "if Pre_model == \"ResNet50\":\n",
    "    from keras.applications import ResNet50\n",
    "elif Pre_model == \"VGG16\":\n",
    "    from keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Pre_model == \"ResNet50\":\n",
    "       conv_base = ResNet50 ( weights='imagenet',       # Weight checkpoint from which to initialize the model \n",
    "                     include_top = False,        # No include the dense connected classifier on top\n",
    "                     input_shape = (150,150,3))  # Shape of the image tensors to feed in the network\n",
    "\n",
    "elif Pre_model == \"VGG16\":\n",
    "       conv_base = VGG16  ( weights='imagenet',       # Weight checkpoint from which to initialize the model \n",
    "                     include_top = False,        # No include the dense connected classifier on top\n",
    "                     input_shape = (150,150,3))  # Shape of the image tensors to feed in the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See conv Architecture\n",
    "#conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extendind the conv_base model \n",
    "from keras import layers\n",
    "from keras import models\n",
    "model = models.Sequential ()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 4, 4, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 18,910,017\n",
      "Trainable params: 18,910,017\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print Network \n",
    "print_network (results_dir, model, Model_name)\n",
    "#Save Network \n",
    "save_network_json (model_json_dir, model, Model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model before freezing:\n",
      "30\n",
      "Model After freezing:\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Freeze the convolutional base\n",
    "print ( 'Model before freezing:')\n",
    "print(len(model.trainable_weights))\n",
    "conv_base.trainable = False\n",
    "print ( 'Model After freezing:')\n",
    "print(len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train,validation & Test \n",
    "(dftrain, dfval, dftest) = read_dataframes(csv_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbackt to be used \n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "callbacks_list = [\n",
    "#         keras.callbacks.EarlyStopping (\n",
    "#             monitor = 'acc',             # Monitors the accuracy\n",
    "#             patience = 2,),              # Interrupt if acc no improve in 3 epochs\n",
    "\n",
    "#  ModelCheckpoint to store the weights of the best performing epoch. \n",
    "    \n",
    "         keras.callbacks.ModelCheckpoint(filepath=model_bin_dir+\"Best_weights\"+Model_name+\".hdf5\", \n",
    "             monitor = 'val_loss', # Won't overwritte the model file unless val_loss has\n",
    "             verbose=1,            # improve \n",
    "             save_best_only=True),\n",
    "         \n",
    "#         keras.callbacks.TensorBoard(\n",
    "#             log_dir =  Tensor_dir, \n",
    "#            histogram_freq = 0,  ) # No histograms - validation data must be provided as a tensor\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3126,
     "status": "ok",
     "timestamp": 1572217564583,
     "user": {
      "displayName": "Victoria Albors",
      "photoUrl": "",
      "userId": "00745240505511363037"
     },
     "user_tz": -60
    },
    "id": "8UzGZWV_i5mX",
    "outputId": "7543fba5-0b25-4af2-eb57-716cb6ad1bac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2056 validated image filenames belonging to 2 classes.\n",
      "Found 1028 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#Rescale images  1/255\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range =40,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True, ) \n",
    "                                   \n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "#batch size = 20 . In this way \"steps per epoch\" ( how many batches have to been treated before going to \n",
    "# the next epoch  is exact  2000 samples =  20 samples x batch  and steps per epoch = 100 - 1 epoch)\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "        dataframe=dftrain,         # Data frame with info of the files and targets\n",
    "        directory=imag_dir,        # path to the directory to read the images from\n",
    "        x_col='file_name_ext',     # column in the data frame that contains the file names \n",
    "        y_col='bm',                # column in the data frame that has the target data\n",
    "        target_size=(150, 150),    # dimensions that the images will be resized\n",
    "        batch_size=32,             # size of the batches of data (default: 32).\n",
    "        class_mode='binary')       # Mode for yielding the targets:1D numpy array of binary labels\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_dataframe(\n",
    "        dataframe=dfval,           # Data frame with info of the files and targets\n",
    "        directory=imag_dir,        # path to the directory to read the images from\n",
    "        x_col='file_name_ext',     # column in the data frame that contains the file names \n",
    "        y_col='bm',                # column in the data frame that has the target data\n",
    "        target_size=(150, 150),    # dimensions that the images will be resized\n",
    "        batch_size=32,             # size of the batches of data (default: 32).\n",
    "        class_mode='binary')       # Mode for yielding the targets:1D numpy array of binary labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Network \n",
    "from keras import optimizers \n",
    "model.compile ( loss='binary_crossentropy',\n",
    "#               optimizer = optimizers.RMSprop(lr=1e-4),\n",
    "               optimizer = optimizers.Adam(lr=1e-4),\n",
    "               metrics= ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 20824275,
     "status": "ok",
     "timestamp": 1572238403431,
     "user": {
      "displayName": "Victoria Albors",
      "photoUrl": "",
      "userId": "00745240505511363037"
     },
     "user_tz": -60
    },
    "id": "uCoVQU7Xi5mj",
    "outputId": "4f4f09d8-0b79-430f-cdc2-e5e6389a0eac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[32,150,150,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node vgg16/block1_conv2/Relu}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-ed308aa2e5d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#      callbacks=callbacks_list,                 # callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m       validation_steps =32 )                     # nº samples validation / Batch size = 1028 /32\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    214\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[32,150,150,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node vgg16/block1_conv2/Relu}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "num_train = 2100\n",
    "num_val = 700\n",
    "num_test = 700\n",
    "import time\n",
    "\n",
    "if TRAIN :\n",
    "    epochs = 100\n",
    "    start_time = time.time()   \n",
    "    history = model.fit_generator ( \n",
    "      train_generator,\n",
    "      steps_per_epoch =66,                      # nº samples training/ Batch size  = 2100 / 32 \n",
    "      epochs = epochs,\n",
    "#      callbacks=callbacks_list,                 # callbacks\n",
    "      validation_data= validation_generator,\n",
    "      validation_steps =22 )                     # nº samples validation / Batch size = 700 /32\n",
    "\n",
    "\n",
    "\n",
    "    save_model(model, history, model_bin_dir, Model_name)\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    print( time.strftime('Time spent in training :'\"%H:%M:%S\", time.gmtime(elapsed_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Model Test if not need to Train \n",
    "if not TRAIN :\n",
    "    model = model_load ( model_bin_dir, Model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display curves of loss and accuracy during training and save results \n",
    "\n",
    "plot_save_acc_loss(results_dir, history_dic, Model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine Tuning\n",
    "# =========================================================================================================\n",
    "Model_name = \"ModelK6_0_Fine_ResNet50\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Base convolutional layer\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We fine-tune the last three convolutional layers : all layers up tp block4_pool should be frozen.\n",
    "# Layers block5 should be trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable = True\n",
    "set_trainable = False\n",
    "for layer in conv_base.layers:\n",
    "    if layer.name == 'block5_conv1':\n",
    "        set_trainable = True\n",
    "    if layer.name == 'block5_conv2':\n",
    "        set_trainable = True\n",
    "    if layer.name == 'block5_conv3':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "              \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Network \n",
    "from keras import optimizers \n",
    "model.compile ( loss='binary_crossentropy',\n",
    "#               optimizer = optimizers.RMSprop(lr=1e-5),\n",
    "               optimizer = optimizers.Adam(lr=1e-4),\n",
    "               metrics= ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "num_train = 2100\n",
    "num_val = 700\n",
    "num_test = 700\n",
    "if TRAIN :\n",
    "    epochs = 30\n",
    "    start_time = time.time()   \n",
    "    history = model.fit_generator ( \n",
    "      train_generator,\n",
    "      steps_per_epoch =66,                      # nº samples training/ Batch size  = 2100 / 32 \n",
    "      epochs = epochs,\n",
    "#      callbacks=callbacks_list,                 # callbacks\n",
    "      validation_data= validation_generator,\n",
    "      validation_steps =22 )                     # nº samples validation / Batch size = 700 /32\n",
    "\n",
    "\n",
    "    \n",
    "    save_model(model, history, model_bin_dir, Model_name)\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    print( time.strftime('Time spent in training :'\"%H:%M:%S\", time.gmtime(elapsed_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Model Test if not need to Train \n",
    "if not TRAIN :\n",
    "    model = model_load ( model_bin_dir, Model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display curves of loss and accuracy during training and save results \n",
    "\n",
    "plot_save_acc_loss(results_dir, history_dic, Model_name)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Preprocess_Fit2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
