{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ur34g7Fbi5lr"
   },
   "source": [
    "# Notebook- Data Augmentation ( Model K6_1 Fine Tuning VGG16 ) \n",
    "# Author : V.Albors   Date : 08.02.2020\n",
    "# Purpose : Fine Tuning\n",
    "\n",
    "\n",
    "**Input** :  \n",
    "  * CSV files that identify the images to use as train and validation. CSV files are in directory csv_dir   \n",
    "  * Images from train and validation. Images are in directory : imag_dir  \n",
    "  * Saved model. Model is in directory : model_bin_dir  \n",
    "  \n",
    "**Output**:  \n",
    "  * Download of the model trained with train dataset - with Data Augmentation\n",
    "  * Download the history of the model in order to be evaluated \n",
    "\n",
    "**Process**:  \n",
    " * Read Train and Validation images ( identified in the .csv files ) from the imag_dir directory   \n",
    " * Import Pre-Trained Network ( VGG16 or ResNett50 ) \n",
    " \n",
    " \n",
    " * Add custom network on top already trained network\n",
    " * Freeze the base Network\n",
    " * Train the part Added  ( Train with Augmentation + No callback ) \n",
    " \n",
    " * Fine Tune : \n",
    " * Unfreeze some layers in the base network\n",
    " * Jointly train both these layers and the part added ( Train with Augmentation + No callback ) \n",
    " \n",
    " \n",
    " * Save the trained model and history of the model in directory model_bin_dir \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.keras.backend.clear_session()  # Reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.config.experimental.list_physical_devices('GPU') \n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "tf.debugging.set_log_device_placement(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "2.0.0\n",
      "10.0\n",
      "7.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)\n",
    "\n",
    "#Tensorflow version \n",
    "print(tf.__version__)\n",
    "from tensorflow.python.platform import build_info as tf_build_info\n",
    "print(tf_build_info.cuda_version_number)\n",
    "# Cuda Version 9.0 in v1.10.0\n",
    "print(tf_build_info.cudnn_version_number)\n",
    "# CudNN 7 in v1.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the model, directories & if to train the model \n",
    "Pre_model = \"VGG16\"\n",
    "#Pre_model = \"ResNet50\"\n",
    "if Pre_model == \"ResNet50\":\n",
    "    Model_name = \"ModelK6_1_ResNet50\"\n",
    "else:\n",
    "    Model_name = \"ModelK6_1_VGG16\"\n",
    "\n",
    "Model_directory = \"MODELK6\"\n",
    "\n",
    "TRAIN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['confusion_ROC_AUC', 'create_column_tensor', 'create_label_tensor', 'create_val_test', 'define_dirs', 'extract_images_bm', 'extract_images_train', 'load_hist_model', 'load_images', 'load_images_tf', 'model_load', 'plot_save_acc_loss', 'print_network', 'process_clinical_info', 'read_dataframes', 'read_dataframes_tables', 'reproducible_results', 'save_model', 'save_model_no_opt', 'save_network_json', 'start', 'stop', 'to_one_hot', 'to_one_hot_words', 'xi_squared']\n"
     ]
    }
   ],
   "source": [
    "# Import routines\n",
    "import sys  \n",
    "subrc_dir = \"/home/valborsf/Documents/UOC/PFMProject/\"\n",
    "\n",
    "sys.path.append(subrc_dir)  \n",
    "from  Models_routines import *\n",
    "import inspect\n",
    "\n",
    "# List functions inside the module\n",
    "import Models_routines as module\n",
    "functions = inspect.getmembers(module, inspect.isfunction)\n",
    "lsfunctions = [item[0] for item in functions]\n",
    "print ( lsfunctions )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories\n",
    "(root_dir,json_dir,imag_dir,csv_dir,model_json_dir,model_bin_dir,results_dir,Tensor_dir) = define_dirs(Model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New dataset without SONIC disturbing images\n",
    "json_dir =  root_dir +\"/DataNew/ALL_JSON/\"                # .json dir images\n",
    "imag_dir =  root_dir +\"/DataNew/ALL_IMAGES/\"              # .png dir - images\n",
    "\n",
    "# directories for  CSV's\n",
    "csv_dir =  root_dir +\"/DataNew4/CSV/\"                      # .csv dir - dftrain, dfval, dftest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Pre_model == \"ResNet50\":\n",
    "    from keras.applications import ResNet50\n",
    "elif Pre_model == \"VGG16\":\n",
    "    from keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Pre_model == \"ResNet50\":\n",
    "       conv_base = ResNet50 ( weights='imagenet',       # Weight checkpoint from which to initialize the model \n",
    "                     include_top = False,        # No include the dense connected classifier on top\n",
    "                     input_shape = (150,150,3))  # Shape of the image tensors to feed in the network\n",
    "\n",
    "elif Pre_model == \"VGG16\":\n",
    "       conv_base = VGG16  ( weights='imagenet',       # Weight checkpoint from which to initialize the model \n",
    "                     include_top = False,        # No include the dense connected classifier on top\n",
    "                     input_shape = (150,150,3))  # Shape of the image tensors to feed in the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See conv Architecture\n",
    "#conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extendind the conv_base model \n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential ()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print Network \n",
    "print_network (results_dir, model, Model_name)\n",
    "#Save Network \n",
    "save_network_json (model_json_dir, model, Model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 4, 4, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 18,910,017\n",
      "Trainable params: 18,910,017\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model before freezing:\n",
      "30\n",
      "Model After freezing:\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Freeze the convolutional base\n",
    "print ( 'Model before freezing:')\n",
    "print(len(model.trainable_weights))\n",
    "conv_base.trainable = False\n",
    "print ( 'Model After freezing:')\n",
    "print(len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer name: input_1 False\n",
      "layer name: block1_conv1 False\n",
      "layer name: block1_conv2 False\n",
      "layer name: block1_pool False\n",
      "layer name: block2_conv1 False\n",
      "layer name: block2_conv2 False\n",
      "layer name: block2_pool False\n",
      "layer name: block3_conv1 False\n",
      "layer name: block3_conv2 False\n",
      "layer name: block3_conv3 False\n",
      "layer name: block3_pool False\n",
      "layer name: block4_conv1 False\n",
      "layer name: block4_conv2 False\n",
      "layer name: block4_conv3 False\n",
      "layer name: block4_pool False\n",
      "layer name: block5_conv1 False\n",
      "layer name: block5_conv2 False\n",
      "layer name: block5_conv3 False\n",
      "layer name: block5_pool False\n"
     ]
    }
   ],
   "source": [
    "#### VERY IMPORTANT !!!!\n",
    "#### Set the layers as trainable or not, otherwise when loading the model doesn't recover the optimizer parameters\n",
    "#### and error when compiling the optimizer\n",
    "####\n",
    "\n",
    "conv_base.trainable = False\n",
    "set_trainable = False\n",
    "for layer in conv_base.layers:\n",
    "    \n",
    "    if layer.name == ' ':\n",
    "        set_trainable = False\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "    print ( \"layer name:\", layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 4, 4, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 18,910,017\n",
      "Trainable params: 4,195,329\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Network \n",
    "#from tensorflow.keras import optimizers\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model.compile ( loss='binary_crossentropy',\n",
    "#               optimizer = optimizers.RMSprop(lr=1e-4),\n",
    "               optimizer = Adam(lr=1e-4),\n",
    "               metrics= ['acc'])\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train,validation & Test \n",
    "(dftrain, dfval, dftest) = read_dataframes(csv_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbackt to be used \n",
    "\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "callbacks_list = [\n",
    "#         keras.callbacks.EarlyStopping (\n",
    "#             monitor = 'acc',             # Monitors the accuracy\n",
    "#             patience = 2,),              # Interrupt if acc no improve in 3 epochs\n",
    "\n",
    "#  ModelCheckpoint to store the weights of the best performing epoch. \n",
    "    \n",
    "         keras.callbacks.ModelCheckpoint(filepath=model_bin_dir+\"Best_weights\"+Model_name+\".hdf5\", \n",
    "             monitor = 'val_loss', # Won't overwritte the model file unless val_loss has\n",
    "             verbose=1,            # improve \n",
    "             save_best_only=True),\n",
    "         \n",
    "#         keras.callbacks.TensorBoard(\n",
    "#             log_dir =  Tensor_dir, \n",
    "#            histogram_freq = 0,  ) # No histograms - validation data must be provided as a tensor\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3126,
     "status": "ok",
     "timestamp": 1572217564583,
     "user": {
      "displayName": "Victoria Albors",
      "photoUrl": "",
      "userId": "00745240505511363037"
     },
     "user_tz": -60
    },
    "id": "8UzGZWV_i5mX",
    "outputId": "7543fba5-0b25-4af2-eb57-716cb6ad1bac"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#Rescale images  1/255\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range =40,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True, ) \n",
    "                                   \n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "#batch size = 20 . In this way \"steps per epoch\" ( how many batches have to been treated before going to \n",
    "# the next epoch  is exact  2000 samples =  20 samples x batch  and steps per epoch = 100 - 1 epoch)\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "        dataframe=dftrain,         # Data frame with info of the files and targets\n",
    "        directory=imag_dir,        # path to the directory to read the images from\n",
    "        x_col='file_name_ext',     # column in the data frame that contains the file names \n",
    "        y_col='bm',                # column in the data frame that has the target data\n",
    "        target_size=(150, 150),    # dimensions that the images will be resized\n",
    "        batch_size=32,             # size of the batches of data (default: 32).\n",
    "        class_mode='binary')       # Mode for yielding the targets:1D numpy array of binary labels\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_dataframe(\n",
    "        dataframe=dfval,           # Data frame with info of the files and targets\n",
    "        directory=imag_dir,        # path to the directory to read the images from\n",
    "        x_col='file_name_ext',     # column in the data frame that contains the file names \n",
    "        y_col='bm',                # column in the data frame that has the target data\n",
    "        target_size=(150, 150),    # dimensions that the images will be resized\n",
    "        batch_size=32,             # size of the batches of data (default: 32).\n",
    "        class_mode='binary')       # Mode for yielding the targets:1D numpy array of binary labels\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "        dataframe=dftest,         # Data frame with info of the files and targets\n",
    "        directory=imag_dir,        # path to the directory to read the images from\n",
    "        x_col='file_name_ext',     # column in the data frame that contains the file names \n",
    "        y_col='bm',                # column in the data frame that has the target data\n",
    "        target_size=(150, 150),    # dimensions that the images will be resized\n",
    "        batch_size=32,             # size of the batches of data (default: 32).\n",
    "        shuffle=False,             # IMPORTANT !!! Do not shuffle test data !!!!!!!!!!!!!\n",
    "        class_mode='binary')       # Mode for yielding the targets:1D numpy array of binary labels\n",
    "#        class_mode=None)       # Mode for yielding the targets:1D numpy array of binary labels\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 20824275,
     "status": "ok",
     "timestamp": 1572238403431,
     "user": {
      "displayName": "Victoria Albors",
      "photoUrl": "",
      "userId": "00745240505511363037"
     },
     "user_tz": -60
    },
    "id": "uCoVQU7Xi5mj",
    "outputId": "4f4f09d8-0b79-430f-cdc2-e5e6389a0eac"
   },
   "outputs": [],
   "source": [
    "num_train = 2100\n",
    "num_val = 700\n",
    "num_test = 700\n",
    "import time\n",
    "\n",
    "if TRAIN :\n",
    "    epochs = 100\n",
    "    start_time = time.time()   \n",
    "    history = model.fit_generator ( \n",
    "      train_generator,\n",
    "      steps_per_epoch =66,                      # nº samples training/ Batch size  = 2100 / 32 \n",
    "      epochs = epochs,\n",
    "      callbacks=callbacks_list,                 # callbacks\n",
    "      validation_data= validation_generator,\n",
    "      validation_steps =22 )                     # nº samples validation / Batch size = 700 /32\n",
    "\n",
    "\n",
    "#    This was need when the layers where not marked as trainable and I get an error when loading the model\n",
    "#    and compiling\n",
    "#    save_model_no_opt(model, history, model_bin_dir, Model_name)   # save model without optimizer \n",
    "    save_model (model, history, model_bin_dir, Model_name)   # save model without optimizer\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    print( time.strftime('Time spent in training :'\"%H:%M:%S\", time.gmtime(elapsed_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Model Test if not need to Train \n",
    "TRAIN = True\n",
    "if not TRAIN :\n",
    "    model = model_load ( model_bin_dir, Model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display curves of loss and accuracy during training and save results \n",
    "\n",
    "plot_save_acc_loss(results_dir, history.history, Model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model \n",
    "# Load weights \n",
    "\n",
    "#model = build_model()\n",
    "#model.load_weights('my_weights.model')\n",
    "TRAIN = False\n",
    "if not TRAIN :\n",
    "    Model_name = \"ModelK6_1_VGG16\"\n",
    "#    tf.keras.backend.clear_session()  # Reset\n",
    "    model = model_load ( model_bin_dir, Model_name)                     # we load the model without optimizer\n",
    "    print ( \"Load Weights: \")\n",
    "    model.load_weights(model_bin_dir+\"Best_weights\"+Model_name+\".hdf5\")\n",
    "    \n",
    "#  This was added when layers were not marked as trainable and I get error \n",
    "#    model.compile ( loss='binary_crossentropy',                        # we need to compiled after \n",
    "#               optimizer = Adam(lr=1e-4),\n",
    "#               metrics= ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the test generator\n",
    "test_generator.reset()\n",
    "scores = model.evaluate(test_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predictions\n",
    "# This takes time !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "y_pred_keras = model.predict(test_generator).ravel()       # y_pred_probabilities\n",
    "y_pred = y_pred_keras > 0.5                                # y_pred_class : if >0.5  = True => Malignant\n",
    "y_test = test_generator.classes                            # Ground truth\n",
    "class_labels = list(test_generator.class_indices.keys())   # Class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print Metrics + Confusion ROC AUC\n",
    "confusion_ROC_AUC ( y_test, y_pred, y_pred_keras, class_labels, results_dir, Model_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine Tuning\n",
    "# =========================================================================================================\n",
    "Model_name = \"ModelK6_1_Fine_VGG16\"\n",
    "Model_directory = \"MODELK6\"\n",
    "\n",
    "TRAIN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset Data Generators \n",
    "train_generator.reset()\n",
    "validation_generator.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base convolutional layer\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We fine-tune the last three convolutional layers : all layers up tp block4_pool should be frozen.\n",
    "# Layers block5 should be trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable = True\n",
    "set_trainable = False\n",
    "for layer in conv_base.layers:\n",
    "    \n",
    "    if layer.name == 'block5_conv1':\n",
    "        set_trainable = True\n",
    "    if layer.name == 'block5_conv2':\n",
    "        set_trainable = True\n",
    "    if layer.name == 'block5_conv3':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "    print ( \"layer name:\", layer.name, layer.trainable)\n",
    "              \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ( 'Model Trainable weights:')\n",
    "print(len(model.trainable_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Network \n",
    "from keras.optimizers import Adam\n",
    "# very low learning rate - to limit the magnitude of the modifications  of the fine tuned layers \n",
    "\n",
    "model.compile ( loss='binary_crossentropy',\n",
    "#               optimizer = optimizers.RMSprop(lr=1e-4),\n",
    "               optimizer = Adam(lr=1e-5),\n",
    "               metrics= ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important when fitting again if we do not use another call back list the modelchekpoint doesn't \n",
    "# save the file with the proper name, but the previous file name from the previous model \n",
    "# Callbackt to be used \n",
    "\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "callbacks_list2 = [\n",
    "#         keras.callbacks.EarlyStopping (\n",
    "#             monitor = 'acc',             # Monitors the accuracy\n",
    "#             patience = 2,),              # Interrupt if acc no improve in 3 epochs\n",
    "\n",
    "#  ModelCheckpoint to store the weights of the best performing epoch. \n",
    "    \n",
    "         keras.callbacks.ModelCheckpoint(filepath=model_bin_dir+\"Best_weights\"+Model_name+\".hdf5\", \n",
    "             monitor = 'val_loss', # Won't overwritte the model file unless val_loss has\n",
    "             verbose=1,            # improve \n",
    "             save_best_only=True),\n",
    "         \n",
    "#         keras.callbacks.TensorBoard(\n",
    "#             log_dir =  Tensor_dir, \n",
    "#            histogram_freq = 0,  ) # No histograms - validation data must be provided as a tensor\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "num_train = 2100\n",
    "num_val = 700\n",
    "num_test = 700\n",
    "\n",
    "if TRAIN :\n",
    "    epochs = 100\n",
    "    start_time = time.time()   \n",
    "    history = model.fit_generator ( \n",
    "      train_generator,\n",
    "      steps_per_epoch =66,                      # nº samples training/ Batch size  = 2100 / 32 \n",
    "      epochs = epochs,\n",
    "      callbacks=callbacks_list2,                 # callbacks\n",
    "      validation_data= validation_generator,\n",
    "      validation_steps =22 )                     # nº samples validation / Batch size = 700 /32\n",
    "\n",
    "\n",
    "    \n",
    "    save_model(model, history, model_bin_dir, Model_name)\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    print( time.strftime('Time spent in training :'\"%H:%M:%S\", time.gmtime(elapsed_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Model Test if not need to Train \n",
    "if not TRAIN :\n",
    "    model = model_load ( model_bin_dir, Model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display curves of loss and accuracy during training and save results \n",
    "\n",
    "plot_save_acc_loss(results_dir, history.history, Model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model \n",
    "# Load weights \n",
    "\n",
    "#model = build_model()\n",
    "#model.load_weights('my_weights.model')\n",
    "TRAIN = False\n",
    "if not TRAIN :\n",
    "\n",
    "    Model_name = \"ModelK6_1_Fine_VGG16\"\n",
    "\n",
    "    model = model_load ( model_bin_dir, Model_name)\n",
    "    print ( \"Load Weights :\")\n",
    "    model.load_weights(model_bin_dir+\"Best_weights\"+Model_name+\".hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4 - ROC /AUC\n",
    "#Evaluate your model  -> Evaluation number -> nº samples/ size batch times\n",
    "#scores = model.evaluate(test_generator, verbose=1, steps = 31)\n",
    "scores = model.evaluate(test_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss = 52 % \n",
    "# Accuracy of 75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predictions\n",
    "# This takes time !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "y_pred_keras = model.predict(test_generator).ravel()       # y_pred_probabilities\n",
    "y_pred = y_pred_keras > 0.5                                # y_pred_class : if >0.5  = True => Malignant\n",
    "y_test = test_generator.classes                            # Ground truth\n",
    "class_labels = list(test_generator.class_indices.keys())   # Class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print Metrics + Confusion ROC AUC\n",
    "confusion_ROC_AUC ( y_test, y_pred, y_pred_keras, class_labels, results_dir, Model_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()  # Reset"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Preprocess_Fit2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "“gpu2”",
   "language": "python",
   "name": "gpu2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
