{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.keras.backend.clear_session()  # Reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copiado paper como se calcula el cost con TF y Keras  ( La he manteniddo pero es la misma que abajo porque esta tiene la derivada)\n",
    "def compute_stable_bce_cost_derivative(Y, Z):\n",
    "    \"\"\"\n",
    "    This function computes the \"Stable\" Binary Cross-Entropy(stable_bce) Cost and returns the Cost and its\n",
    "    derivative w.r.t Z_last(the last linear node) .\n",
    "    The Stable Binary Cross-Entropy Cost is defined as:\n",
    "    => (1/m) * np.sum(max(Z,0) - ZY + log(1+exp(-|Z|)))\n",
    "    Args:\n",
    "        Y: labels of data\n",
    "        Z: Values from the last linear node\n",
    "    Returns:\n",
    "        cost: The \"Stable\" Binary Cross-Entropy Cost result\n",
    "        dZ_last: gradient of Cost w.r.t Z_last\n",
    "    \"\"\"\n",
    "    m = Y.shape[0]\n",
    "\n",
    "    cost = (1/m) * np.sum(np.maximum(Z, 0) - Z*Y + np.log(1+ np.exp(- np.abs(Z))))\n",
    "    dZ_last = (1/m) * ((1/(1+np.exp(- Z))) - Y)  # from Z computes the Sigmoid so P_hat - Y, where P_hat = sigma(Z)\n",
    "\n",
    "    return cost, dZ_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Formato de Tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stable_bce_cost(Y, Z):\n",
    "    \"\"\"\n",
    "    This function computes the \"Stable\" Binary Cross-Entropy(stable_bce) Cost and returns the Cost.\n",
    "    The Stable Binary Cross-Entropy Cost is defined as:\n",
    "    => (1/m) * np.sum(max(Z,0) - ZY + log(1+exp(-|Z|)))\n",
    "    Args:\n",
    "        Y: labels of data\n",
    "        Z: Values from the last linear node\n",
    "    Returns:\n",
    "        cost: The \"Stable\" Binary Cross-Entropy Cost result\n",
    "        dZ_last: gradient of Cost w.r.t Z_last\n",
    "    \"\"\"\n",
    "    m = Y.shape[0]\n",
    "    cost = (1/m) * np.sum(np.maximum(Z, 0) - Z*Y + np.log(1+ np.exp(- np.abs(Z))))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9082330465316772\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "Y=  np.array( [0., 0., 1., 1.] , dtype=np.float32)          # Ground truth\n",
    "Z = np.array ( [1., 1., 1., 0.], dtype=np.float32 )         # P probabilities\n",
    "print ( compute_stable_bce_cost(Y, Z) )                     # Estable Binary Cross Entropy from TF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.9082331\n"
     ]
    }
   ],
   "source": [
    "#  Tensorflow  format with function -> tf.keras.losses.BinaryCrossentropy\n",
    "#  When logits = True  (Use Tensorflow way Stable Binary Cross Entropy ) \n",
    "#      Loss ( Y,Z ) = max ( z,0 ) - z*y + log (1 + e ^ |z|)\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "loss = bce([0., 0., 1., 1.], [1., 1., 1., 0.])\n",
    "# Y= [0., 0., 1., 1.] y Z= [1., 1., 1., 0.]\n",
    "print('Loss: ', loss.numpy())                                # Loss: 0.9082331 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9082330607786535\n"
     ]
    }
   ],
   "source": [
    "# Loss calculado en formato Tensorflow Manualmente\n",
    "import numpy as np\n",
    "loss = bce([0., 0., 1., 1.], [1., 1., 1., 0.])\n",
    "# Y= [0., 0., 1., 1.] y Z= [1., 1., 1., 0.]\n",
    "# Calculo manual del loss \n",
    "# Y ground truth Z logits - salida de la Ãºltima neurona pero sin sigmoid\n",
    "# Loss ( Y,Z ) = max ( z,0 ) - z*y + log (1 + e ^ |z|)\n",
    "loss_0_1 = ( 1+np.log(1+np.exp(-1)))    #  y= 0 z=1\n",
    "loss_1_1 = ( 1-1+np.log(1+np.exp(-1)))  #  y= 1 z=1 \n",
    "loss_1_0 = ( np.log(1+1))               #  y= 1 z=0 \n",
    "Total_loss =  loss_0_1 + loss_0_1 + loss_1_1 + loss_1_0 \n",
    "print ( Total_loss / 4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formato de Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bce_keras(Y, P):\n",
    "    \"\"\"\"\n",
    "    Args:\n",
    "        Y: labels of data\n",
    "        P: Probabilities that have been clipped to  min = epsilon ( 10 ^- 7 ) &  max = 1 - epsilon \n",
    "    Returns:\n",
    "        cost: Binary Cross-Entropy Returned by keras        \n",
    "    \"\"\"\n",
    "    m = Y.shape[0]\n",
    "    epsilon =  1e-7 \n",
    "    cost = (1/m) * np.sum( Y * np.log(P+epsilon ) +  ( 1- Y )* np.log ( 1- P +epsilon ) ) \n",
    "    return -cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  11.522857\n"
     ]
    }
   ],
   "source": [
    "# tf.keras.losses.BinaryCrossentropy   logits = False \n",
    "# When logits = False ( p's are probabilities )\n",
    "#      - Clipping P's min 10^-7  y max 1-10^-7\n",
    "#      - Use formula Cross entropy \n",
    "#          L(Y,P) = -Y*ln(P)-(1-Y)*ln(1-P)\n",
    "Y=  np.array( [0., 0., 1., 1.] , dtype=np.float32)                              # Ground truth\n",
    "P = np.array ( [1., 1., 1., 0.], dtype=np.float32 )                             # P probabilities\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "loss = bce( Y , P)\n",
    "print('Loss: ', loss.numpy())  # Loss: 11.522857\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.522856712341309\n"
     ]
    }
   ],
   "source": [
    "# Compute Binary Cross entropy \n",
    "\n",
    "# Numpy uses 64 bit as default \n",
    "# Tensorflow  32 bits as default \n",
    "Y=  np.array( [0., 0., 1., 1.], dtype=np.float32  )                              # Ground truth\n",
    "P = np.array ( [1., 1., 1., 0.], dtype=np.float32  )                             # P probabilities\n",
    "epsilon = 1e-7 \n",
    "P_est = np.clip ( P,epsilon, 1.-epsilon)        # P_est - Estable probabilities\n",
    "print ( compute_bce_keras(Y, P_est)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P est: [9.999999e-01 9.999999e-01 9.999999e-01 1.000000e-07]\n",
      "Z: [ 16.11809555  16.11809555  16.11809555 -16.11809555]\n",
      "<class 'numpy.ndarray'>\n",
      "4\n",
      "12.08857176348192\n",
      "tf.Tensor(12.088571548461914, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "# Esta es la forma que dice como lo hace el paper Keras pero no coincide con los resultados  !!!!!!\n",
    "#  Cuando se utiliza logits = False interpreta el 2ndo vector como probabilidades P\n",
    "#  y las transforma en Z's ( lo que se llama logits ). despues con logits utilizara la formula de Tensorflow\n",
    "#  1. Transformacion de probabilidades a Z's :\n",
    "#      - las P's como min son 10^-7  y max 0.999999\n",
    "#      - Estas P's las transforman en Z con la formula :\n",
    "#           Z = ln( P / ( 1- P))\n",
    "#  2. Utiliza la formula de Tensorflow con Logits \n",
    "#        Loss ( Y,Z ) = max ( z,0 ) - z*y + log (1 + e ^ |z|)\n",
    "\n",
    "# Keras pasa los valores de P a Z's \n",
    "# Y= [0., 0., 1., 1.] y P= [1., 1., 1., 0.]\n",
    "\n",
    "#  1. Transformacion de probabilidades a Z's :\n",
    "#      - las P's como min son 10^-7  y max 0.999999\n",
    "#loss = bce([0., 0., 1., 1.], [0.999999, 0.999999, 0.999999, 10^-7])\n",
    "import numpy as np\n",
    "# Numpy uses 64 bit as default \n",
    "# Tensorflow  32 bits as default \n",
    "Y=  np.array( [0., 0., 1., 1.] )                              # Ground truth\n",
    "P = np.array ( [1., 1., 1., 0.] )                             # P probabilities\n",
    "\n",
    "epsilon = np.float_power(10, -7)\n",
    "P_est = np.clip ( P,epsilon, 1.-epsilon)        # P_est - Estable probabilitis\n",
    "\n",
    "print ( 'P est:', P_est)\n",
    "#      - Estas P's las transforman en Z con la formula :\n",
    "#           Z = ln( P / ( 1- P))\n",
    "Z= np.log( P_est / ( 1- P_est))                              # Transfor Probabilities to Logits\n",
    "print ('Z:', Z)                                              # Print Logits\n",
    "\n",
    "#Y_t = tf.convert_to_tensor(Y)\n",
    "print ( type ( Y))\n",
    "print (  Y.shape[0] )\n",
    "# Cross entropy de TF : \n",
    "loss_Y, der_loss = compute_stable_bce_cost(Y, Z) \n",
    "print ( loss_Y )                     # Estable Binary Cross Entropy from TF \n",
    "\n",
    "####\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "loss = bce(Y,Z)\n",
    "print (loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ktf2",
   "language": "python",
   "name": "ktf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
